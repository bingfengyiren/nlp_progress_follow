bert论文翻译
=================
原始论文地址:https://arxiv.org/pdf/1810.04805.pdf

摘要
-----------------
本论文介绍了一种称为bert的新语言模型，该模型表示来源于Transfromer的双向编码表示。不同于其它语言模型的是，bert旨在在所有层的左右上下文上通过联合概率的方式，来预训练未标记文本的深层双向表示形式。因此，仅需要调整模型的输出层就可以在许多任务中创建一个state-of-the-art的模型，比如：问答、语言理解，不需要针对特定任务做太多结构的修改。BERT概念简单，但实践效果非常好。在11个NLP任务中都取得了最好结果。其中，GLUE数据集的评分提升到了80.5%(绝对提升了7.7%)，MultiNLI的精度提升到了86.7%（绝对提升4.6%），SQuAD v1.1的F1值提升到了93.2%（绝对提升1.5个点），SQuAD v2.0的F1提升到了83.1（绝对提升5.1个点）。
