bert论文翻译
=================
原始论文地址:https://arxiv.org/pdf/1810.04805.pdf

摘要
-----------------
本论文介绍了一种称为bert的新语言模型，该模型表示来源于Transfromer的双向编码表示。不同于其它
语言模型的是，bert旨在在所有层的左右上下文上通过联合概率的方式，来预训练未标记文本的深层双向表
示形式。因此，仅需要调整模型的输出层就可以在许多任务中创建一个state-of-the-art的模型，比
如：问答、语言理解，不需要针对特定任务做太多结构的修改。
BERT概念简单，但实践效果非常好。在11个NLP任务中都取得了最好结果。其中，GLUE数据集的评分提升到了
80.5%(绝对提升了7.7%)，MultiNLI的精度提升到了86.7%（绝对提升4.6%），SQuAD v1.1的F1值提升到
了93.2%（绝对提升1.5个点），SQuAD v2.0的F1提升到了83.1（绝对提升5.1个点）。

引言
-----------------
语言模型预训练已被证明可以有效地改善许多自然语言处理任务。包括句子级的任务，比如自然语言推断和释义，
旨在通过对句子间的关系进行分析来预测它们之间的联系，以及词级别的任务，比如命名实体识别和问答，又需要
模型有更细粒度的输出。

将语言模型应用于下游任务的方式有两种：feature-based和fine-tuning。

我们认为当前的技术限制了预训练表示的能力，特别是对于fine-tuning的方式。主要限制的点在于标准语言模型
是单向的，而这限制了在预训练过程中模型的选择。比如说，在OpenAI GPT中，作者使用了一种从左到右的一种结构，
这样在Transformer的self-attention层每个词就只能参与到它前面词的计算。这种限制对于句子级别的任务影响
还相对小一些，但是当对词级别的任务微调时就影响非常大，比如问答，双向上下文就显得尤为重要。

这篇论文中，我们通过BERT提升了基于fine-tuning的方法。BERT受到Closze任务的启发，通过引入MLM(masked language model)
的预训练目标，来缓解前面提到的单向限制。MLM随机掩盖输入中的若干词，而目标是基于上下文预测出原始的单词。
不同于从左到右的预训练语言模型，MLM

证明了双向预训练在语言模型表示中的重要性。不同于Radford等人的单向预训练语言模型，BERT通过MLM预训练深度双向语言表示。和Peters等人独立使用从左到有，从右到左的浅层连接也不相同。
预训练表示降低了特定任务体系结构设计的需求。BERT是首个基于fine-tuning表示的模型，在许多的句子和词级别的任务上都获得了最好的效果，优于许多任务的特定结构。
BERT在11个NLP任务上获取的最好结果。代码仓库：https://github.com/ google-research/bert。
